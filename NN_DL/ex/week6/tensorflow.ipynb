{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the error :\n",
    "# tensorflow has no attribute 'placeholder'\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0\n"
    }
   ],
   "source": [
    "#coefficients = np.array([[1.], [-10.], [25.]])\n",
    "coefficients = np.array([[1.], [-20.], [100.]])\n",
    "\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "x= tf1.placeholder(tf.float32, [3,1])\n",
    "\n",
    "#cost = tf.add(tf.add(w**2, tf.multiply(-10., w)), 25)\n",
    "#cost = w*2 - 10*w + 25\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "train = tf1.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0\n"
    }
   ],
   "source": [
    "session = tf1.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0\n"
    }
   ],
   "source": [
    "with tf1.Session() as session:\n",
    "    session.run(init)\n",
    "    print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.19999999\n"
    }
   ],
   "source": [
    "#session.run(train)\n",
    "session.run(train, feed_dict = {x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9.999977\n"
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    #session.run(train)\n",
    "    session.run(train, feed_dict = {x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Help on class GradientDescentOptimizer in module tensorflow.python.training.gradient_descent:\n\nclass GradientDescentOptimizer(tensorflow.python.training.optimizer.Optimizer)\n |  GradientDescentOptimizer(learning_rate, use_locking=False, name='GradientDescent')\n |  \n |  Optimizer that implements the gradient descent algorithm.\n |  \n |  Method resolution order:\n |      GradientDescentOptimizer\n |      tensorflow.python.training.optimizer.Optimizer\n |      tensorflow.python.training.tracking.base.Trackable\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, learning_rate, use_locking=False, name='GradientDescent')\n |      Construct a new gradient descent optimizer.\n |      \n |      Args:\n |        learning_rate: A Tensor or a floating point value.  The learning\n |          rate to use.\n |        use_locking: If True use locks for update operations.\n |        name: Optional name prefix for the operations created when applying\n |          gradients. Defaults to \"GradientDescent\".\n |      \n |      @compatibility(eager)\n |      When eager execution is enabled, `learning_rate` can be a callable that\n |      takes no arguments and returns the actual value to use. This can be useful\n |      for changing these values across different invocations of optimizer\n |      functions.\n |      @end_compatibility\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n |  \n |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n |      Apply gradients to variables.\n |      \n |      This is the second part of `minimize()`. It returns an `Operation` that\n |      applies gradients.\n |      \n |      Args:\n |        grads_and_vars: List of (gradient, variable) pairs as returned by\n |          `compute_gradients()`.\n |        global_step: Optional `Variable` to increment by one after the\n |          variables have been updated.\n |        name: Optional name for the returned operation.  Default to the\n |          name passed to the `Optimizer` constructor.\n |      \n |      Returns:\n |        An `Operation` that applies the specified gradients. If `global_step`\n |        was not None, that operation also increments `global_step`.\n |      \n |      Raises:\n |        TypeError: If `grads_and_vars` is malformed.\n |        ValueError: If none of the variables have gradients.\n |        RuntimeError: If you should use `_distributed_apply()` instead.\n |  \n |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, grad_loss=None)\n |      Compute gradients of `loss` for the variables in `var_list`.\n |      \n |      This is the first part of `minimize()`.  It returns a list\n |      of (gradient, variable) pairs where \"gradient\" is the gradient\n |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n |      `IndexedSlices`, or `None` if there is no gradient for the\n |      given variable.\n |      \n |      Args:\n |        loss: A Tensor containing the value to minimize or a callable taking\n |          no arguments which returns the value to minimize. When eager execution\n |          is enabled it must be a callable.\n |        var_list: Optional list or tuple of `tf.Variable` to update to minimize\n |          `loss`.  Defaults to the list of variables collected in the graph\n |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n |        gate_gradients: How to gate the computation of gradients.  Can be\n |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n |        aggregation_method: Specifies the method used to combine gradient terms.\n |          Valid values are defined in the class `AggregationMethod`.\n |        colocate_gradients_with_ops: If True, try colocating gradients with\n |          the corresponding op.\n |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n |      \n |      Returns:\n |        A list of (gradient, variable) pairs. Variable is always present, but\n |        gradient can be `None`.\n |      \n |      Raises:\n |        TypeError: If `var_list` contains anything else than `Variable` objects.\n |        ValueError: If some arguments are invalid.\n |        RuntimeError: If called with eager execution enabled and `loss` is\n |          not callable.\n |      \n |      @compatibility(eager)\n |      When eager execution is enabled, `gate_gradients`, `aggregation_method`,\n |      and `colocate_gradients_with_ops` are ignored.\n |      @end_compatibility\n |  \n |  get_name(self)\n |  \n |  get_slot(self, var, name)\n |      Return a slot named `name` created for `var` by the Optimizer.\n |      \n |      Some `Optimizer` subclasses use additional variables.  For example\n |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n |      gives access to these `Variable` objects if for some reason you need them.\n |      \n |      Use `get_slot_names()` to get the list of slot names created by the\n |      `Optimizer`.\n |      \n |      Args:\n |        var: A variable passed to `minimize()` or `apply_gradients()`.\n |        name: A string.\n |      \n |      Returns:\n |        The `Variable` for the slot if it was created, `None` otherwise.\n |  \n |  get_slot_names(self)\n |      Return a list of the names of slots created by the `Optimizer`.\n |      \n |      See `get_slot()`.\n |      \n |      Returns:\n |        A list of strings.\n |  \n |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n |      Add operations to minimize `loss` by updating `var_list`.\n |      \n |      This method simply combines calls `compute_gradients()` and\n |      `apply_gradients()`. If you want to process the gradient before applying\n |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n |      of using this function.\n |      \n |      Args:\n |        loss: A `Tensor` containing the value to minimize.\n |        global_step: Optional `Variable` to increment by one after the\n |          variables have been updated.\n |        var_list: Optional list or tuple of `Variable` objects to update to\n |          minimize `loss`.  Defaults to the list of variables collected in\n |          the graph under the key `GraphKeys.TRAINABLE_VARIABLES`.\n |        gate_gradients: How to gate the computation of gradients.  Can be\n |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n |        aggregation_method: Specifies the method used to combine gradient terms.\n |          Valid values are defined in the class `AggregationMethod`.\n |        colocate_gradients_with_ops: If True, try colocating gradients with\n |          the corresponding op.\n |        name: Optional name for the returned operation.\n |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n |      \n |      Returns:\n |        An Operation that updates the variables in `var_list`.  If `global_step`\n |        was not `None`, that operation also increments `global_step`.\n |      \n |      Raises:\n |        ValueError: If some of the variables are not `Variable` objects.\n |      \n |      @compatibility(eager)\n |      When eager execution is enabled, `loss` should be a Python function that\n |      takes no arguments and computes the value to be minimized. Minimization (and\n |      gradient computation) is done with respect to the elements of `var_list` if\n |      not None, else with respect to any trainable variables created during the\n |      execution of the `loss` function. `gate_gradients`, `aggregation_method`,\n |      `colocate_gradients_with_ops` and `grad_loss` are ignored when eager\n |      execution is enabled.\n |      @end_compatibility\n |  \n |  variables(self)\n |      A list of variables which encode the current state of `Optimizer`.\n |      \n |      Includes slot variables and additional global variables created by the\n |      optimizer in the current default graph.\n |      \n |      Returns:\n |        A list of variables.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n |  \n |  GATE_GRAPH = 2\n |  \n |  GATE_NONE = 0\n |  \n |  GATE_OP = 1\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
    }
   ],
   "source": [
    "help(tf1.train.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}